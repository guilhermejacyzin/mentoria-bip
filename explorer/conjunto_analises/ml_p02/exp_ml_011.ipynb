{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfdd15d",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'C:\\Users\\JacyzinGuilherme(Bip)\\mentoria-bip\\dados_editados'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 631\u001b[39m\n\u001b[32m    626\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------\u001b[39;00m\n\u001b[32m    627\u001b[39m \u001b[38;5;66;03m# D) Salve o resultado final em CSV (para Power BI)\u001b[39;00m\n\u001b[32m    628\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------\u001b[39;00m\n\u001b[32m    630\u001b[39m saida_csv = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mJacyzinGuilherme(Bip)\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmentoria-bip\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdados_editados\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmetricas_por_cidade_xgb_v1.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m \u001b[43mdf_metricas_cidades\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43msaida_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------\u001b[39;00m\n\u001b[32m    634\u001b[39m \u001b[38;5;66;03m# E) Mostre as primeiras linhas no console\u001b[39;00m\n\u001b[32m    635\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------\u001b[39;00m\n\u001b[32m    637\u001b[39m \u001b[38;5;28mprint\u001b[39m(df_metricas_cidades.head(\u001b[32m10\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JacyzinGuilherme(Bip\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JacyzinGuilherme(Bip\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\generic.py:3989\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3978\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3980\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3981\u001b[39m     frame=df,\n\u001b[32m   3982\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3986\u001b[39m     decimal=decimal,\n\u001b[32m   3987\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3989\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3990\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3991\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3992\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3993\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3994\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3995\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3996\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4006\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JacyzinGuilherme(Bip\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[43mcsv_formatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JacyzinGuilherme(Bip\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mCreate the writer & save.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JacyzinGuilherme(Bip\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\io\\common.py:749\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression != \u001b[33m\"\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JacyzinGuilherme(Bip\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\io\\common.py:616\u001b[39m, in \u001b[36mcheck_parent_directory\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    614\u001b[39m parent = Path(path).parent\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent.is_dir():\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot save file into a non-existent directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Cannot save file into a non-existent directory: 'C:\\Users\\JacyzinGuilherme(Bip)\\mentoria-bip\\dados_editados'"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SCRIPT COMPLETO (REFEITO): XGBoost por cidade + Tuning por cidade\n",
    "# SEM SMOTE | COM TRATAMENTO DE COLUNAS CATEGÓRICAS (ONE-HOT)\n",
    "#\n",
    "# OBJETIVO:\n",
    "# - Treinar 1 modelo XGBoost para cada cidade (coluna \"localidade\")\n",
    "# - Respeitar o tempo de cada cidade (split temporal por cidade)\n",
    "# - Fazer tuning de hiperparâmetros por cidade (Random Search)\n",
    "# - Fazer validação temporal dentro do treino (TimeSeriesSplit)\n",
    "# - Ajustar automaticamente desbalanceamento com scale_pos_weight\n",
    "# - Escolher threshold no treino (opcional)\n",
    "# - Avaliar no teste e salvar métricas\n",
    "# - Exportar CSV final (1 linha por cidade) para usar no Power BI\n",
    "#\n",
    "# IMPORTANTE:\n",
    "# - Este script evita o erro \"dtype object\" no XGBoost transformando\n",
    "#   colunas texto/categóricas em one-hot (pd.get_dummies).\n",
    "# - Também alinha colunas entre treino e teste, para não faltar dummy.\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1) IMPORTS (bibliotecas)\n",
    "# ----------------------------\n",
    "\n",
    "# Pandas: leitura e manipulação de tabelas\n",
    "import pandas as pd\n",
    "\n",
    "# Numpy: operações numéricas\n",
    "import numpy as np\n",
    "\n",
    "# JSON: para salvar hiperparâmetros como texto no CSV final\n",
    "import json\n",
    "\n",
    "# ParameterSampler: sorteia combinações aleatórias de hiperparâmetros (Random Search)\n",
    "# TimeSeriesSplit: validação cruzada respeitando ordem temporal (sem embaralhar)\n",
    "from sklearn.model_selection import ParameterSampler, TimeSeriesSplit\n",
    "\n",
    "# Métricas: para avaliar desempenho de classificação\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "# Modelo: XGBoost Classifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) FUNÇÃO: limpar e preparar dataset (converte data e ordena)\n",
    "# ============================================================\n",
    "\n",
    "def preparar_dataset(df, col_data=\"data\", col_cidade=\"localidade\"):\n",
    "    \"\"\"\n",
    "    Esta função:\n",
    "    1) Faz uma cópia do DataFrame para não alterar o original.\n",
    "    2) Converte a coluna de data para datetime.\n",
    "    3) Remove linhas onde a data ficou inválida (NaT).\n",
    "    4) Ordena por cidade e data para manter consistência temporal.\n",
    "    5) Retorna o DataFrame pronto.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cria uma cópia do DataFrame original\n",
    "    df_local = df.copy()\n",
    "\n",
    "    # Converte a coluna de data para datetime; erros viram NaT\n",
    "    df_local[col_data] = pd.to_datetime(df_local[col_data], errors=\"coerce\")\n",
    "\n",
    "    # Remove linhas que não têm data válida\n",
    "    df_local = df_local.dropna(subset=[col_data])\n",
    "\n",
    "    # Ordena por cidade e data para garantir ordem temporal\n",
    "    df_local = df_local.sort_values([col_cidade, col_data]).reset_index(drop=True)\n",
    "\n",
    "    # Retorna DataFrame pronto\n",
    "    return df_local\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) FUNÇÃO: gerar X e y de uma cidade + one-hot encoding\n",
    "# ============================================================\n",
    "\n",
    "def montar_X_y_com_onehot(df_cidade, feature_cols, col_target):\n",
    "    \"\"\"\n",
    "    Esta função:\n",
    "    1) Separa X (features) e y (target).\n",
    "    2) Identifica colunas object (texto).\n",
    "    3) Converte essas colunas para one-hot via get_dummies.\n",
    "    4) Retorna X (já numérico) e y (int).\n",
    "    \"\"\"\n",
    "\n",
    "    # X recebe apenas as colunas de features\n",
    "    X = df_cidade[feature_cols].copy()\n",
    "\n",
    "    # y recebe a coluna target e garante tipo int (0/1)\n",
    "    y = df_cidade[col_target].astype(int).copy()\n",
    "\n",
    "    # Identifica colunas de texto (dtype object)\n",
    "    cols_obj = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "    # Se existirem colunas texto, faz one-hot nelas\n",
    "    # dummy_na=True cria uma categoria para valores nulos (NaN)\n",
    "    if len(cols_obj) > 0:\n",
    "        X = pd.get_dummies(\n",
    "            X,\n",
    "            columns=cols_obj,\n",
    "            dummy_na=True,\n",
    "            drop_first=False\n",
    "        )\n",
    "\n",
    "    # Retorna X e y\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) FUNÇÃO: split temporal (treino/teste) por cidade\n",
    "# ============================================================\n",
    "\n",
    "def split_temporal(X, y, proporcao_treino=0.8):\n",
    "    \"\"\"\n",
    "    Esta função:\n",
    "    1) Calcula um índice de corte baseado na proporção.\n",
    "    2) Separa treino e teste respeitando a ordem temporal.\n",
    "    3) Retorna X_train, y_train, X_test, y_test.\n",
    "    \"\"\"\n",
    "\n",
    "    # Total de linhas\n",
    "    n_total = len(X)\n",
    "\n",
    "    # Índice de corte (por exemplo 80% do total)\n",
    "    idx_corte = int(np.floor(proporcao_treino * n_total))\n",
    "\n",
    "    # Treino recebe as primeiras linhas\n",
    "    X_train = X.iloc[:idx_corte].reset_index(drop=True)\n",
    "    y_train = y.iloc[:idx_corte].reset_index(drop=True)\n",
    "\n",
    "    # Teste recebe as últimas linhas\n",
    "    X_test = X.iloc[idx_corte:].reset_index(drop=True)\n",
    "    y_test = y.iloc[idx_corte:].reset_index(drop=True)\n",
    "\n",
    "    # Retorna conjuntos\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) FUNÇÃO: alinhar colunas entre treino e teste (pós one-hot)\n",
    "# ============================================================\n",
    "\n",
    "def alinhar_colunas_treino_teste(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Esta função garante que:\n",
    "    - X_train e X_test tenham exatamente as mesmas colunas.\n",
    "    - Se uma coluna existir em um e não no outro, ela é criada com valor 0.\n",
    "    \"\"\"\n",
    "\n",
    "    # align(..., join=\"left\") garante que X_test tenha pelo menos as colunas do treino\n",
    "    # fill_value=0 preenche colunas ausentes com zeros\n",
    "    X_train_al, X_test_al = X_train.align(X_test, join=\"left\", axis=1, fill_value=0)\n",
    "\n",
    "    # Retorna alinhados\n",
    "    return X_train_al, X_test_al\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) FUNÇÃO: calcular scale_pos_weight (desbalanceamento)\n",
    "# ============================================================\n",
    "\n",
    "def calcular_scale_pos_weight(y):\n",
    "    \"\"\"\n",
    "    scale_pos_weight = (n_negativos / n_positivos)\n",
    "    Isso ajuda o XGBoost a não ignorar a classe minoritária.\n",
    "    \"\"\"\n",
    "\n",
    "    # Conta negativos e positivos\n",
    "    neg = int((y == 0).sum())\n",
    "    pos = int((y == 1).sum())\n",
    "\n",
    "    # Evita divisão por zero\n",
    "    if pos == 0:\n",
    "        return 1.0\n",
    "\n",
    "    # Retorna a razão\n",
    "    return float(neg / pos)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) FUNÇÃO: calcular métricas no teste\n",
    "# ============================================================\n",
    "\n",
    "def calcular_metricas_classificacao(y_true, y_proba, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Recebe:\n",
    "    - y_true: labels reais (0/1)\n",
    "    - y_proba: probabilidades previstas para classe 1\n",
    "    - threshold: corte para virar classe\n",
    "\n",
    "    Retorna:\n",
    "    - métricas (precision, recall, f1, roc_auc, pr_auc)\n",
    "    - TP/TN/FP/FN\n",
    "    \"\"\"\n",
    "\n",
    "    # Converte probabilidade em classe com o threshold\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "    # Matriz de confusão: TN, FP, FN, TP\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "\n",
    "    # Métricas baseadas em classe\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    # Métricas baseadas em probabilidade (precisa ter as duas classes)\n",
    "    if len(np.unique(y_true)) == 2:\n",
    "        roc_auc = roc_auc_score(y_true, y_proba)\n",
    "        pr_auc = average_precision_score(y_true, y_proba)\n",
    "    else:\n",
    "        roc_auc = np.nan\n",
    "        pr_auc = np.nan\n",
    "\n",
    "    # Retorna dicionário de métricas\n",
    "    return {\n",
    "        \"threshold\": float(threshold),\n",
    "        \"tp\": int(tp),\n",
    "        \"tn\": int(tn),\n",
    "        \"fp\": int(fp),\n",
    "        \"fn\": int(fn),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1\": float(f1),\n",
    "        \"roc_auc\": float(roc_auc) if roc_auc == roc_auc else np.nan,\n",
    "        \"pr_auc\": float(pr_auc) if pr_auc == pr_auc else np.nan\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8) FUNÇÃO: escolher threshold no treino (opcional)\n",
    "# ============================================================\n",
    "\n",
    "def escolher_threshold_no_treino(y_true, y_proba, metrica=\"f1\"):\n",
    "    \"\"\"\n",
    "    Esta função tenta vários thresholds e escolhe o melhor baseado em:\n",
    "    - f1 (padrão), ou\n",
    "    - recall, ou\n",
    "    - precision\n",
    "\n",
    "    Ela testa thresholds de 0.10 até 0.90, passo 0.05\n",
    "    \"\"\"\n",
    "\n",
    "    # Cria lista de thresholds\n",
    "    thresholds = np.round(np.arange(0.10, 0.91, 0.05), 2)\n",
    "\n",
    "    # Define melhor threshold e melhor valor\n",
    "    melhor_thr = 0.5\n",
    "    melhor_val = -np.inf\n",
    "\n",
    "    # Testa cada threshold\n",
    "    for thr in thresholds:\n",
    "        # Calcula métricas com esse threshold\n",
    "        m = calcular_metricas_classificacao(y_true, y_proba, threshold=thr)\n",
    "\n",
    "        # Pega a métrica escolhida\n",
    "        val = m.get(metrica, np.nan)\n",
    "\n",
    "        # Atualiza se for melhor\n",
    "        if val == val and val > melhor_val:\n",
    "            melhor_val = val\n",
    "            melhor_thr = thr\n",
    "\n",
    "    # Retorna melhor threshold encontrado\n",
    "    return float(melhor_thr)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9) FUNÇÃO: avaliar hiperparâmetros com validação temporal\n",
    "# ============================================================\n",
    "\n",
    "def avaliar_params_com_validacao_temporal(X_train, y_train, params, n_splits=4, scoring=\"pr_auc\", random_state=42):\n",
    "    \"\"\"\n",
    "    Para um conjunto de hiperparâmetros (params), esta função:\n",
    "    1) Divide X_train/y_train em folds temporais com TimeSeriesSplit.\n",
    "    2) Treina em cada fold de treino e avalia no fold de validação.\n",
    "    3) Calcula a métrica escolhida (scoring) e guarda.\n",
    "    4) Retorna a média dos scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cria objeto de validação temporal\n",
    "    tss = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    # Lista de scores de cada fold\n",
    "    scores = []\n",
    "\n",
    "    # Para cada divisão temporal\n",
    "    for idx_tr, idx_val in tss.split(X_train):\n",
    "        # Separa treino do fold\n",
    "        X_tr = X_train.iloc[idx_tr]\n",
    "        y_tr = y_train.iloc[idx_tr]\n",
    "\n",
    "        # Separa validação do fold\n",
    "        X_val = X_train.iloc[idx_val]\n",
    "        y_val = y_train.iloc[idx_val]\n",
    "\n",
    "        # Se treino ou validação tiver apenas 1 classe, pule o fold\n",
    "        if len(np.unique(y_tr)) < 2 or len(np.unique(y_val)) < 2:\n",
    "            continue\n",
    "\n",
    "        # Calcula scale_pos_weight no fold de treino\n",
    "        spw = calcular_scale_pos_weight(y_tr)\n",
    "\n",
    "        # Cria modelo com params do trial\n",
    "        model = XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            tree_method=\"hist\",\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "            scale_pos_weight=spw,\n",
    "            **params\n",
    "        )\n",
    "\n",
    "        # Treina no fold de treino\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        # Prediz probabilidade no fold de validação\n",
    "        y_proba_val = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        # Calcula score conforme métrica escolhida\n",
    "        if scoring == \"pr_auc\":\n",
    "            score = average_precision_score(y_val, y_proba_val)\n",
    "        elif scoring == \"roc_auc\":\n",
    "            score = roc_auc_score(y_val, y_proba_val)\n",
    "        else:\n",
    "            score = average_precision_score(y_val, y_proba_val)\n",
    "\n",
    "        # Guarda score desse fold\n",
    "        scores.append(score)\n",
    "\n",
    "    # Se não houve fold válido, retorne -inf (inválido)\n",
    "    if len(scores) == 0:\n",
    "        return -np.inf\n",
    "\n",
    "    # Retorna média dos scores\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 10) FUNÇÃO PRINCIPAL: treinar por cidade com tuning\n",
    "# ============================================================\n",
    "\n",
    "def treinar_xgb_por_cidade_com_tuning(\n",
    "    df,\n",
    "    col_cidade=\"localidade\",\n",
    "    col_data=\"data\",\n",
    "    col_target=\"chove_amanha_vtr\",\n",
    "    proporcao_treino=0.8,\n",
    "    min_linhas_cidade=500,\n",
    "    n_iter=50,\n",
    "    n_splits_tss=4,\n",
    "    scoring_valid=\"pr_auc\",\n",
    "    usar_threshold_otimo=True,\n",
    "    metrica_threshold=\"f1\",\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Função principal que:\n",
    "    - prepara dataset (data datetime + ordenação)\n",
    "    - itera por cidade\n",
    "    - monta X e y com one-hot\n",
    "    - split temporal\n",
    "    - tuning com random search\n",
    "    - treina modelo final com melhores params\n",
    "    - escolhe threshold no treino (opcional)\n",
    "    - avalia no teste\n",
    "    - salva 1 linha de métricas por cidade\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepara dataset (data datetime e ordenação)\n",
    "    df_local = preparar_dataset(df, col_data=col_data, col_cidade=col_cidade)\n",
    "\n",
    "    # Define colunas que não são features\n",
    "    colunas_nao_features = {col_cidade, col_data, col_target}\n",
    "\n",
    "    # Define lista de features como todas as colunas exceto as não-features\n",
    "    feature_cols = [c for c in df_local.columns if c not in colunas_nao_features]\n",
    "\n",
    "    # Define espaço de hiperparâmetros (realista, sem explodir)\n",
    "    param_distributions = {\n",
    "        \"n_estimators\": [200, 400, 600, 800],\n",
    "        \"learning_rate\": [0.01, 0.03, 0.05, 0.1],\n",
    "        \"max_depth\": [2, 3, 4, 5, 6],\n",
    "        \"subsample\": [0.7, 0.8, 0.9, 1.0],\n",
    "        \"colsample_bytree\": [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        \"min_child_weight\": [1, 3, 5, 7, 10],\n",
    "        \"reg_lambda\": [1.0, 2.0, 5.0, 10.0],\n",
    "        \"gamma\": [0, 0.5, 1.0, 2.0]\n",
    "    }\n",
    "\n",
    "    # Gera combinações aleatórias de hiperparâmetros\n",
    "    sampler = ParameterSampler(\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iter,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Lista para guardar resultados finais\n",
    "    resultados = []\n",
    "\n",
    "    # Lista de cidades únicas\n",
    "    cidades = df_local[col_cidade].dropna().unique()\n",
    "\n",
    "    # Itera por cidade\n",
    "    for cidade in cidades:\n",
    "        # Filtra dados da cidade\n",
    "        df_cid = df_local[df_local[col_cidade] == cidade].copy()\n",
    "\n",
    "        # Conta total de linhas\n",
    "        n_total = int(len(df_cid))\n",
    "\n",
    "        # Se tiver poucas linhas, marca e continua\n",
    "        if n_total < min_linhas_cidade:\n",
    "            resultados.append({\n",
    "                \"cidade\": cidade,\n",
    "                \"cidade_insuficiente\": True,\n",
    "                \"motivo\": f\"n_total < {min_linhas_cidade}\",\n",
    "                \"n_total\": n_total\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Ordena por data dentro da cidade\n",
    "        df_cid = df_cid.sort_values(col_data).reset_index(drop=True)\n",
    "\n",
    "        # Monta X e y já com one-hot\n",
    "        X, y = montar_X_y_com_onehot(df_cid, feature_cols=feature_cols, col_target=col_target)\n",
    "\n",
    "        # Se total tiver apenas uma classe, não dá para treinar\n",
    "        if len(np.unique(y)) < 2:\n",
    "            resultados.append({\n",
    "                \"cidade\": cidade,\n",
    "                \"cidade_insuficiente\": True,\n",
    "                \"motivo\": \"apenas_uma_classe_no_total\",\n",
    "                \"n_total\": n_total\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Split temporal\n",
    "        X_train, y_train, X_test, y_test = split_temporal(X, y, proporcao_treino=proporcao_treino)\n",
    "\n",
    "        # Alinha colunas de treino e teste (pós one-hot)\n",
    "        X_train, X_test = alinhar_colunas_treino_teste(X_train, X_test)\n",
    "\n",
    "        # Se treino ou teste tiver apenas uma classe, não dá para avaliar\n",
    "        if len(np.unique(y_train)) < 2 or len(np.unique(y_test)) < 2:\n",
    "            resultados.append({\n",
    "                \"cidade\": cidade,\n",
    "                \"cidade_insuficiente\": True,\n",
    "                \"motivo\": \"treino_ou_teste_sem_duas_classes\",\n",
    "                \"n_total\": n_total,\n",
    "                \"n_treino\": int(len(y_train)),\n",
    "                \"n_teste\": int(len(y_test))\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Variáveis para melhor resultado\n",
    "        melhor_score = -np.inf\n",
    "        melhor_params = None\n",
    "\n",
    "        # Tuning: testa combinações de hiperparâmetros\n",
    "        for trial_id, params in enumerate(sampler):\n",
    "            score = avaliar_params_com_validacao_temporal(\n",
    "                X_train=X_train,\n",
    "                y_train=y_train,\n",
    "                params=params,\n",
    "                n_splits=n_splits_tss,\n",
    "                scoring=scoring_valid,\n",
    "                random_state=random_state\n",
    "            )\n",
    "\n",
    "            # Atualiza se for melhor\n",
    "            if score > melhor_score:\n",
    "                melhor_score = score\n",
    "                melhor_params = params\n",
    "\n",
    "        # Se não achou trial válido\n",
    "        if melhor_params is None or melhor_score == -np.inf:\n",
    "            resultados.append({\n",
    "                \"cidade\": cidade,\n",
    "                \"cidade_insuficiente\": True,\n",
    "                \"motivo\": \"nenhum_trial_valido\",\n",
    "                \"n_total\": n_total\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Calcula scale_pos_weight no treino inteiro\n",
    "        spw_final = calcular_scale_pos_weight(y_train)\n",
    "\n",
    "        # Cria modelo final com melhores parâmetros\n",
    "        model_final = XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            tree_method=\"hist\",\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "            scale_pos_weight=spw_final,\n",
    "            **melhor_params\n",
    "        )\n",
    "\n",
    "        # Treina modelo final\n",
    "        model_final.fit(X_train, y_train)\n",
    "\n",
    "        # Probabilidade no treino (para escolher threshold)\n",
    "        y_proba_train = model_final.predict_proba(X_train)[:, 1]\n",
    "\n",
    "        # Probabilidade no teste (para métricas finais)\n",
    "        y_proba_test = model_final.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Escolhe threshold no treino (opcional)\n",
    "        if usar_threshold_otimo:\n",
    "            thr_escolhido = escolher_threshold_no_treino(\n",
    "                y_true=y_train,\n",
    "                y_proba=y_proba_train,\n",
    "                metrica=metrica_threshold\n",
    "            )\n",
    "        else:\n",
    "            thr_escolhido = 0.5\n",
    "\n",
    "        # Calcula métricas no teste\n",
    "        metricas_teste = calcular_metricas_classificacao(\n",
    "            y_true=y_test,\n",
    "            y_proba=y_proba_test,\n",
    "            threshold=thr_escolhido\n",
    "        )\n",
    "\n",
    "        # Informações temporais: ano início e ano fim\n",
    "        ano_inicio = int(df_cid[col_data].dt.year.min())\n",
    "        ano_fim = int(df_cid[col_data].dt.year.max())\n",
    "\n",
    "        # Prevalência de chuva no total e no teste\n",
    "        preval_total = float(y.mean())\n",
    "        preval_teste = float(y_test.mean())\n",
    "\n",
    "        # Tamanho do teste\n",
    "        n_teste = int(len(y_test))\n",
    "\n",
    "        # Nível de confiabilidade baseado no tamanho do teste\n",
    "        if n_teste < 200:\n",
    "            confiabilidade = \"baixa\"\n",
    "        elif n_teste < 800:\n",
    "            confiabilidade = \"media\"\n",
    "        else:\n",
    "            confiabilidade = \"alta\"\n",
    "\n",
    "        # Número final de features (após one-hot)\n",
    "        n_features = int(X_train.shape[1])\n",
    "\n",
    "        # Salva resultado final da cidade\n",
    "        resultados.append({\n",
    "            \"cidade\": cidade,\n",
    "            \"cidade_insuficiente\": False,\n",
    "            \"motivo\": \"\",\n",
    "            \"ano_inicio\": ano_inicio,\n",
    "            \"ano_fim\": ano_fim,\n",
    "            \"n_total\": n_total,\n",
    "            \"n_treino\": int(len(y_train)),\n",
    "            \"n_teste\": n_teste,\n",
    "            \"n_features\": n_features,\n",
    "            \"prevalencia_total\": preval_total,\n",
    "            \"prevalencia_teste\": preval_teste,\n",
    "            \"confiabilidade\": confiabilidade,\n",
    "            \"scoring_valid\": scoring_valid,\n",
    "            \"best_valid_score\": float(melhor_score),\n",
    "            \"best_params_json\": json.dumps(melhor_params, ensure_ascii=False),\n",
    "            **metricas_teste\n",
    "        })\n",
    "\n",
    "    # Converte lista de resultados em DataFrame\n",
    "    df_resultados = pd.DataFrame(resultados)\n",
    "\n",
    "    # Retorna DataFrame final\n",
    "    return df_resultados\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 11) EXECUÇÃO (MAIN)\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --------------------------------------------------------\n",
    "    # A) Defina o caminho do arquivo Excel\n",
    "    # --------------------------------------------------------\n",
    "    # IMPORTANTE:\n",
    "    # - Use r\"\" para evitar problemas com barras invertidas no Windows.\n",
    "    # - Corrija o caminho conforme seu PC.\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    caminho_excel = r\"C:\\Users\\JacyzinGuilherme(Bip\\mentoria-bip\\dados_editados\\australia_clima_v9.xlsx\"\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # B) Leia o Excel para um DataFrame\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    df = pd.read_excel(caminho_excel)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # C) Rode o treinamento por cidade com tuning\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    df_metricas_cidades = treinar_xgb_por_cidade_com_tuning(\n",
    "        df=df,\n",
    "        col_cidade=\"localidade\",\n",
    "        col_data=\"data\",\n",
    "        col_target=\"chove_amanha_vtr\",\n",
    "        proporcao_treino=0.8,\n",
    "        min_linhas_cidade=500,\n",
    "        n_iter=50,               # Aumente para 100+ se quiser mais busca (demora mais)\n",
    "        n_splits_tss=4,\n",
    "        scoring_valid=\"pr_auc\",  # bom para desbalanceamento\n",
    "        usar_threshold_otimo=True,\n",
    "        metrica_threshold=\"f1\",\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # D) Salve o resultado final em CSV (para Power BI)\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    #saida_csv = r\"C:\\Users\\JacyzinGuilherme(Bip\\mentoria-bip\\dados_editados\\metricas_por_cidade_xgb_v1.csv\"\n",
    "    #df_metricas_cidades.to_csv(saida_csv, index=False)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # E) Mostre as primeiras linhas no console\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    print(df_metricas_cidades)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c973fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
