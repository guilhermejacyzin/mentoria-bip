{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3a1681a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports e instalações\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#!pip install scikit-learn\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# Modelos\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# JSON: para salvar hiperparâmetros como texto no CSV final\n",
    "import json\n",
    "\n",
    "# Métricas\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix, classification_report, f1_score, average_precision_score)\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# ParameterSampler para sorteio de combinações aleatórias de hiperparâmetros\n",
    "# TimeSeriesSplit é para validação cruzada respeitando ordem temporal\n",
    "from sklearn.model_selection import ParameterSampler, TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b9af18d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparar_dataset(df, col_data=\"data\", col_cidade=\"localidade\"):\n",
    "    \"\"\"\n",
    "    Função:\n",
    "    Faz uma cópia do DataFrame para não alterar o original.\n",
    "    Converte a coluna de data para datetime.\n",
    "    Remove linhas onde a data ficou inválida (NaT).\n",
    "    Ordena por cidade e data para manter consistência temporal.\n",
    "    Retorna o DataFrame pronto.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cria uma cópia do DataFrame original\n",
    "    df_local = df.copy()\n",
    "\n",
    "    # Converte a coluna de data para datetime; erros viram NaT\n",
    "    df_local[col_data] = pd.to_datetime(df_local[col_data], errors=\"coerce\")\n",
    "\n",
    "    # Remove linhas que não têm data válida\n",
    "    df_local = df_local.dropna(subset=[col_data])\n",
    "\n",
    "    # Ordena por cidade e data para garantir ordem temporal\n",
    "    df_local = df_local.sort_values([col_cidade, col_data]).reset_index(drop=True)\n",
    "\n",
    "    # Retorna DataFrame pronto\n",
    "    return df_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "add8b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def montar_X_y_com_onehot(df_cidade, feature_cols, col_target):\n",
    "    \"\"\"\n",
    "    Função:\n",
    "    Separa X (features) e y (target).\n",
    "    Identifica colunas object (texto).\n",
    "    Converte essas colunas para one-hot via get_dummies.\n",
    "    Retorna X (já numérico) e y (int).\n",
    "    \"\"\"\n",
    "\n",
    "    # X recebe apenas as colunas de features\n",
    "    X = df_cidade[feature_cols].copy()\n",
    "\n",
    "    # y recebe a coluna target e garante tipo int (0/1)\n",
    "    y = df_cidade[col_target].astype(int).copy()\n",
    "\n",
    "    # Identifica colunas de texto (dtype object)\n",
    "    cols_obj = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "    # Se existirem colunas texto, faz one-hot nelas\n",
    "    # dummy_na=True cria uma categoria para valores nulos (NaN)\n",
    "    if len(cols_obj) > 0:\n",
    "        X = pd.get_dummies(\n",
    "            X,\n",
    "            columns=cols_obj,\n",
    "            dummy_na=True,\n",
    "            drop_first=False\n",
    "        )\n",
    "        \n",
    "        # Retorna X e y\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f2c031e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_temporal(X, y, proporcao_treino=0.8):\n",
    "    \"\"\"\n",
    "    Função:\n",
    "    Calcula um índice de corte baseado na proporção.\n",
    "    Separa treino e teste respeitando a ordem temporal.\n",
    "    Retorna X_train, y_train, X_test, y_test.\n",
    "    \"\"\"\n",
    "\n",
    "    # Total de linhas\n",
    "    n_total = len(X)\n",
    "\n",
    "    # Índice de corte ***por exemplo 80% do total\n",
    "    idx_corte = int(np.floor(proporcao_treino * n_total))\n",
    "\n",
    "    # Treino recebe as primeiras linhas\n",
    "    X_train = X.iloc[:idx_corte].reset_index(drop=True)\n",
    "    y_train = y.iloc[:idx_corte].reset_index(drop=True)\n",
    "\n",
    "    # Teste recebe as últimas linhas\n",
    "    X_test = X.iloc[idx_corte:].reset_index(drop=True)\n",
    "    y_test = y.iloc[idx_corte:].reset_index(drop=True)\n",
    "\n",
    "    # Retorna conjuntos\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6b1daaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alinhar_colunas_treino_teste(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Esta função garante que:\n",
    "    - X_train e X_test tenham exatamente as mesmas colunas.\n",
    "    - Se uma coluna existir em um e não no outro, ela é criada com valor 0.\n",
    "    \"\"\"\n",
    "\n",
    "    # align(..., join=\"left\") garante que X_test tenha pelo menos as colunas do treino\n",
    "    # fill_value=0 preenche colunas ausentes com zeros\n",
    "    X_train_al, X_test_al = X_train.align(X_test, join=\"left\", axis=1, fill_value=0)\n",
    "\n",
    "    # Retorna alinhados\n",
    "    return X_train_al, X_test_al\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bc8699a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_scale_pos_weight(y):\n",
    "    \"\"\"\n",
    "    scale_pos_weight = (n_negativos / n_positivos)\n",
    "    Isso ajuda o XGBoost a não ignorar a classe minoritária.\n",
    "    \"\"\"\n",
    "\n",
    "    # Conta negativos e positivos\n",
    "    neg = int((y == 0).sum())\n",
    "    pos = int((y == 1).sum())\n",
    "\n",
    "    # Evita divisão por zero\n",
    "    if pos == 0:\n",
    "        return 1.0\n",
    "\n",
    "    # Retorna a razão\n",
    "    return float(neg / pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "02186ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_metricas_classificacao(y_true, y_proba, threshold=0.5):\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    if len(np.unique(y_true)) == 2:\n",
    "        roc_auc = roc_auc_score(y_true, y_proba)\n",
    "        pr_auc = average_precision_score(y_true, y_proba)\n",
    "    else:\n",
    "        roc_auc = np.nan\n",
    "        pr_auc = np.nan\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(threshold),\n",
    "        \"tp\": int(tp),\n",
    "        \"tn\": int(tn),\n",
    "        \"fp\": int(fp),\n",
    "        \"fn\": int(fn),\n",
    "        \"accuracy\": float(accuracy),   # <<< CHAVE QUE ESTAVA FALTANDO\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1\": float(f1),\n",
    "        \"roc_auc\": float(roc_auc) if roc_auc == roc_auc else np.nan,\n",
    "        \"pr_auc\": float(pr_auc) if pr_auc == pr_auc else np.nan\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a04a6e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def escolher_threshold_no_treino(y_true, y_proba, metrica=\"f1\"):\n",
    "    \n",
    "    Esta função tenta vários thresholds e escolhe o melhor baseado em:\n",
    "    - f1\n",
    "    - recall\n",
    "    - precision\n",
    "\n",
    "    Ela testa thresholds de 0.10 até 0.90, passo 0.05\n",
    "    \n",
    "\n",
    "    # Cria lista de thresholds\n",
    "    thresholds = np.round(np.arange(0.10, 0.91, 0.05), 2)\n",
    "\n",
    "    # Define melhor threshold e melhor valor\n",
    "    melhor_thr = 0.5\n",
    "    melhor_val = -np.inf\n",
    "\n",
    "    # Testa cada threshold\n",
    "    for thr in thresholds:\n",
    "        # Calcula métricas com esse threshold\n",
    "        m = calcular_metricas_classificacao(y_true, y_proba, threshold=thr)\n",
    "\n",
    "        # Pega a métrica escolhida\n",
    "        val = m.get(metrica, np.nan)\n",
    "\n",
    "        # Atualiza se for melhor\n",
    "        if val == val and val > melhor_val:\n",
    "            melhor_val = val\n",
    "            melhor_thr = thr\n",
    "\n",
    "    # Retorna melhor threshold encontrado\n",
    "    return float(melhor_thr)\n",
    "\"\"\"\n",
    "\n",
    "def escolher_threshold_no_treino(y_true, y_proba, metrica=\"f1\", alpha_recall=0.5):\n",
    "    \"\"\"\n",
    "    Esta função tenta vários thresholds e escolhe o melhor baseado em uma métrica.\n",
    "    Testa thresholds de 0.10 até 0.90 a cada 0.05 passos\n",
    "    Métricas:\n",
    "    - \"f1\"\n",
    "    - \"recall\"\n",
    "    - \"precision\"\n",
    "    - \"accuracy\"\n",
    "    - \"recall_accuracy\" -> score composto:\n",
    "          score = alpha_recall*recall + (1-alpha_recall)*accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    thresholds = np.round(np.arange(0.10, 0.91, 0.05), 2)\n",
    "\n",
    "    melhor_thr = 0.5\n",
    "    melhor_val = -np.inf\n",
    "\n",
    "    for thr in thresholds:\n",
    "        m = calcular_metricas_classificacao(y_true, y_proba, threshold=thr)\n",
    "\n",
    "        # Se a métrica for composta calcula o result\n",
    "        if metrica == \"recall_accuracy\":\n",
    "            val = (alpha_recall * m[\"recall\"]) + ((1.0 - alpha_recall) * m[\"accuracy\"])\n",
    "        else:\n",
    "            val = m.get(metrica, np.nan)\n",
    "\n",
    "        if val == val and val > melhor_val:\n",
    "            melhor_val = val\n",
    "            melhor_thr = thr\n",
    "\n",
    "    return float(melhor_thr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1b4fd35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_params_com_validacao_temporal(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    params,\n",
    "    n_splits=4,\n",
    "    scoring=\"recall_accuracy\",\n",
    "    alpha_recall=0.5,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    scoring=\"recall_accuracy\" => score = alpha_recall*recall + (1-alpha_recall)*accuracy\n",
    "    - threshold 0.5 na validação para comparar hiperparâmetros de forma consistente.\n",
    "    \"\"\"\n",
    "\n",
    "    tss = TimeSeriesSplit(n_splits=n_splits)\n",
    "    scores = []\n",
    "\n",
    "    for idx_tr, idx_val in tss.split(X_train):\n",
    "        X_tr = X_train.iloc[idx_tr]\n",
    "        y_tr = y_train.iloc[idx_tr]\n",
    "\n",
    "        X_val = X_train.iloc[idx_val]\n",
    "        y_val = y_train.iloc[idx_val]\n",
    "\n",
    "        if len(np.unique(y_tr)) < 2 or len(np.unique(y_val)) < 2:\n",
    "            continue\n",
    "\n",
    "        spw = calcular_scale_pos_weight(y_tr)\n",
    "\n",
    "        model = XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            tree_method=\"hist\",\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "            scale_pos_weight=spw,\n",
    "            **params\n",
    "        )\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        y_proba_val = model.predict_proba(X_val)[:, 1]\n",
    "        y_pred_val = (y_proba_val >= 0.5).astype(int)\n",
    "\n",
    "        # Métricas base para o score\n",
    "        rec = recall_score(y_val, y_pred_val, zero_division=0)\n",
    "        acc = accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "        if scoring == \"recall_accuracy\":\n",
    "            score = (alpha_recall * rec) + ((1.0 - alpha_recall) * acc)\n",
    "        elif scoring == \"recall\":\n",
    "            score = rec\n",
    "        elif scoring == \"accuracy\":\n",
    "            score = acc\n",
    "        elif scoring == \"pr_auc\":\n",
    "            score = average_precision_score(y_val, y_proba_val)\n",
    "        elif scoring == \"roc_auc\":\n",
    "            score = roc_auc_score(y_val, y_proba_val)\n",
    "        else:\n",
    "            score = (alpha_recall * rec) + ((1.0 - alpha_recall) * acc)\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    if len(scores) == 0:\n",
    "        return -np.inf\n",
    "\n",
    "    return float(np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171a6cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinar_xgb_por_cidade_com_tuning(\n",
    "    df,\n",
    "    col_cidade=\"localidade\",\n",
    "    col_data=\"data\",\n",
    "    col_target=\"chove_amanha_vtr\",\n",
    "    proporcao_treino=0.8,\n",
    "    min_linhas_cidade=500,\n",
    "    n_iter=50,\n",
    "    n_splits_tss=4,\n",
    "    scoring_valid=\"pr_auc\",\n",
    "    usar_threshold_otimo=True,\n",
    "    metrica_threshold=\"f1\",\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Função objetivo\n",
    "    - prepara dataset \n",
    "    - itera por cidade\n",
    "    - monta X e y com one-hot\n",
    "    - split temporal\n",
    "    - tuning com random search\n",
    "    - treina modelo final com melhores params\n",
    "    - escolhe threshold no treino (opcional)\n",
    "    - avalia no teste\n",
    "    - salva 1 linha de resultados/métricas por cidade\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepara dataset ****data datetime e ordenação\n",
    "    df_local = preparar_dataset(df, col_data=col_data, col_cidade=col_cidade)\n",
    "\n",
    "    # Define colunas que não são features\n",
    "    colunas_nao_features = {col_cidade, col_data, col_target}\n",
    "\n",
    "    # Define lista de features como todas as colunas exceto as não-features\n",
    "    feature_cols = [c for c in df_local.columns if c not in colunas_nao_features]\n",
    "\n",
    "    # Define espaço de hiperparâmetros \n",
    "    param_distributions = {\n",
    "        \"n_estimators\": [200, 400, 600, 800],\n",
    "        \"learning_rate\": [0.01, 0.03, 0.05, 0.1],\n",
    "        \"max_depth\": [2, 3, 4, 5, 6],\n",
    "        \"subsample\": [0.7, 0.8, 0.9, 1.0],\n",
    "        \"colsample_bytree\": [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        \"min_child_weight\": [1, 3, 5, 7, 10],\n",
    "        \"reg_lambda\": [1.0, 2.0, 5.0, 10.0],\n",
    "        \"gamma\": [0, 0.5, 1.0, 2.0]\n",
    "    }\n",
    "\n",
    "    # Gera combinações aleatórias de hiperparâmetros\n",
    "    sampler = ParameterSampler(\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iter,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    #scoring_valid=\"recall\"\n",
    "    #metrica_threshold=\"recall\"\n",
    "\n",
    "    # Lista para guardar resultados finais\n",
    "    resultados = []\n",
    "\n",
    "    # Lista de cidades únicas\n",
    "    cidades = df_local[col_cidade].dropna().unique()\n",
    "\n",
    "    # Itera por cidade\n",
    "    for cidade in cidades:\n",
    "        # Filtra dados da cidade\n",
    "        df_cid = df_local[df_local[col_cidade] == cidade].copy()\n",
    "\n",
    "        # Conta total de linhas\n",
    "        n_total = int(len(df_cid))\n",
    "\n",
    "        # Se tiver poucas linhas, marca e continua\n",
    "        if n_total < min_linhas_cidade:\n",
    "            resultados.append({\n",
    "                \"cidade\": cidade,\n",
    "                \"cidade_insuficiente\": True,\n",
    "                \"motivo\": f\"n_total < {min_linhas_cidade}\",\n",
    "                \"n_total\": n_total\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Ordena por data dentro da cidade\n",
    "        df_cid = df_cid.sort_values(col_data).reset_index(drop=True)\n",
    "\n",
    "        # Monta X e y já com one-hot\n",
    "        X, y = montar_X_y_com_onehot(df_cid, feature_cols=feature_cols, col_target=col_target)\n",
    "\n",
    "        # Se total tiver apenas uma classe, não dá para treinar\n",
    "        if len(np.unique(y)) < 2:\n",
    "            resultados.append({\n",
    "                \"cidade\": cidade,\n",
    "                \"cidade_insuficiente\": True,\n",
    "                \"motivo\": \"apenas_uma_classe_no_total\",\n",
    "                \"n_total\": n_total\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Split temporal\n",
    "        X_train, y_train, X_test, y_test = split_temporal(X, y, proporcao_treino=proporcao_treino)\n",
    "\n",
    "        # Alinha colunas de treino e teste \n",
    "        X_train, X_test = alinhar_colunas_treino_teste(X_train, X_test)\n",
    "\n",
    "        # Se treino ou teste tiver apenas uma classe, não dá para avaliar\n",
    "        if len(np.unique(y_train)) < 2 or len(np.unique(y_test)) < 2:\n",
    "            resultados.append({\n",
    "                \"cidade\": cidade,\n",
    "                \"cidade_insuficiente\": True,\n",
    "                \"motivo\": \"treino_ou_teste_sem_duas_classes\",\n",
    "                \"n_total\": n_total,\n",
    "                \"n_treino\": int(len(y_train)),\n",
    "                \"n_teste\": int(len(y_test))\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Variáveis para melhor resultado\n",
    "        melhor_score = -np.inf\n",
    "        melhor_params = None\n",
    "\n",
    "        # \"Tuning\" para testar combinações de hiperparâmetros\n",
    "        for trial_id, params in enumerate(sampler):\n",
    "            score = avaliar_params_com_validacao_temporal(\n",
    "                X_train=X_train,\n",
    "                y_train=y_train,\n",
    "                params=params,\n",
    "                n_splits=n_splits_tss,\n",
    "                scoring=\"recall_accuracy\",\n",
    "                alpha_recall=0.5,   # 50% recall, 50% accuracy\n",
    "                random_state=random_state\n",
    "            )\n",
    "\n",
    "            # Atualiza se for melhor\n",
    "            if score > melhor_score:\n",
    "                melhor_score = score\n",
    "                melhor_params = params\n",
    "\n",
    "        # Se não achou trial válido\n",
    "        if melhor_params is None or melhor_score == -np.inf:\n",
    "            resultados.append({\n",
    "                \"cidade\": cidade,\n",
    "                \"cidade_insuficiente\": True,\n",
    "                \"motivo\": \"nenhum_trial_valido\",\n",
    "                \"n_total\": n_total\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Calcula scale_pos_weight no treino inteiro\n",
    "        spw_final = calcular_scale_pos_weight(y_train)\n",
    "\n",
    "        # Cria modelo final com melhores parâmetros\n",
    "        model_final = XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            tree_method=\"hist\",\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "            scale_pos_weight=spw_final,\n",
    "            **melhor_params\n",
    "        )\n",
    "\n",
    "        # Treina modelo final\n",
    "        model_final.fit(X_train, y_train)\n",
    "\n",
    "        # Probabilidade no treino ****para escolher threshold\n",
    "        y_proba_train = model_final.predict_proba(X_train)[:, 1]\n",
    "\n",
    "        # Probabilidade no teste *****para métricas finais\n",
    "        y_proba_test = model_final.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Escolhe threshold no treino \n",
    "        if usar_threshold_otimo:\n",
    "            thr_escolhido = escolher_threshold_no_treino(\n",
    "                y_true=y_train,\n",
    "                y_proba=y_proba_train,\n",
    "                metrica=metrica_threshold,\n",
    "                alpha_recall=0.5\n",
    "            )\n",
    "        else:\n",
    "            thr_escolhido = 0.5\n",
    "\n",
    "        # Calcula métricas no teste\n",
    "        metricas_teste = calcular_metricas_classificacao(\n",
    "            y_true=y_test,\n",
    "            y_proba=y_proba_test,\n",
    "            threshold=thr_escolhido\n",
    "        )\n",
    "\n",
    "        # Informações temporais: ano início e ano fim\n",
    "        ano_inicio = int(df_cid[col_data].dt.year.min())\n",
    "        ano_fim = int(df_cid[col_data].dt.year.max())\n",
    "\n",
    "        # Prevalência de chuva no total e no teste\n",
    "        preval_total = float(y.mean())\n",
    "        preval_teste = float(y_test.mean())\n",
    "\n",
    "        # Tamanho do teste\n",
    "        n_teste = int(len(y_test))\n",
    "\n",
    "        # Nível de confiabilidade baseado no tamanho do teste\n",
    "        if n_teste < 200:\n",
    "            confiabilidade = \"baixa\"\n",
    "        elif n_teste < 800:\n",
    "            confiabilidade = \"media\"\n",
    "        else:\n",
    "            confiabilidade = \"alta\"\n",
    "\n",
    "        # Número final de features (após one-hot)\n",
    "        n_features = int(X_train.shape[1])\n",
    "\n",
    "        # Salva resultado final da cidade\n",
    "        resultados.append({\n",
    "            \"cidade\": cidade,\n",
    "            \"cidade_insuficiente\": False,\n",
    "            \"motivo\": \"\",\n",
    "            \"ano_inicio\": ano_inicio,\n",
    "            \"ano_fim\": ano_fim,\n",
    "            \"n_total\": n_total,\n",
    "            \"n_treino\": int(len(y_train)),\n",
    "            \"n_teste\": n_teste,\n",
    "            \"n_features\": n_features,\n",
    "            \"prevalencia_total\": preval_total,\n",
    "            \"prevalencia_teste\": preval_teste,\n",
    "            \"confiabilidade\": confiabilidade,\n",
    "            \"scoring_valid\": scoring_valid,\n",
    "            \"best_valid_score\": float(melhor_score),\n",
    "            \"best_params_json\": json.dumps(melhor_params, ensure_ascii=False),\n",
    "            \"score_valid\": float(melhor_score),\n",
    "            \"score_teste\": float((0.5 * metricas_teste[\"recall\"]) + (0.5 * metricas_teste[\"accuracy\"])),\n",
    "            \"alpha_recall\": 0.5,\n",
    "            **metricas_teste\n",
    "        })\n",
    "\n",
    "    # Converte lista de resultados em DataFrame\n",
    "    df_resultados = pd.DataFrame(resultados)\n",
    "\n",
    "    # Retorna DataFrame final\n",
    "    return df_resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "76ba8081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          cidade  cidade_insuficiente motivo  ano_inicio  ano_fim  n_total  \\\n",
      "0   AliceSprings                False               2008     2016     2224   \n",
      "1       Brisbane                False               2008     2017     2979   \n",
      "2         Cairns                False               2008     2016     2444   \n",
      "3       Canberra                False               2007     2012     1092   \n",
      "4          Cobar                False               2009     2010      534   \n",
      "5   CoffsHarbour                False               2009     2014     1383   \n",
      "6         Darwin                False               2008     2017     3063   \n",
      "7         Hobart                False               2008     2017     1944   \n",
      "8      Melbourne                False               2008     2017     4832   \n",
      "9        Mildura                False               2009     2017     2595   \n",
      "10         Moree                False               2009     2016     1915   \n",
      "11  MountGambier                False               2008     2016     2465   \n",
      "12     Nuriootpa                False               2009     2016     2017   \n",
      "13         Perth                False               2008     2017     5943   \n",
      "14      Portland                False               2009     2016     1911   \n",
      "15          Sale                False               2009     2014     1680   \n",
      "16        Sydney                False               2009     2017     4566   \n",
      "17    Townsville                False               2008     2016     2419   \n",
      "18    WaggaWagga                False               2009     2016     2416   \n",
      "19       Woomera                False               2009     2017     1737   \n",
      "\n",
      "    n_treino  n_teste  n_features  prevalencia_total  ...   tp   tn   fp  fn  \\\n",
      "0       1779      445         106           0.084083  ...   24  369   43   9   \n",
      "1       2383      596         106           0.216516  ...  100  353  123  20   \n",
      "2       1955      489         105           0.307283  ...  125  226  111  27   \n",
      "3        873      219         106           0.202381  ...   42  121   39  17   \n",
      "4        427      107         106           0.117978  ...   10   85    5   7   \n",
      "5       1106      277         106           0.313087  ...   50  164   42  21   \n",
      "6       2450      613         106           0.257591  ...  150  342  100  21   \n",
      "7       1555      389         106           0.241770  ...   82  200   92  15   \n",
      "8       3865      967         106           0.229719  ...  195  477  257  38   \n",
      "9       2076      519         106           0.111368  ...   42  386   77  14   \n",
      "10      1532      383         106           0.134726  ...   35  304   36   8   \n",
      "11      1972      493         106           0.296957  ...   95  290   89  19   \n",
      "12      1613      404         106           0.195340  ...   68  241   88   7   \n",
      "13      4754     1189         106           0.196870  ...  202  762  198  27   \n",
      "14      1528      383         106           0.401361  ...  126  179   60  18   \n",
      "15      1344      336         106           0.214286  ...   64  188   71  13   \n",
      "16      3652      914         106           0.247919  ...  204  440  252  18   \n",
      "17      1935      484         106           0.162877  ...   38  384   29  33   \n",
      "18      1932      484         106           0.176738  ...   71  354   49  10   \n",
      "19      1389      348         106           0.074266  ...   17  263   64   4   \n",
      "\n",
      "    accuracy  precision    recall        f1   roc_auc    pr_auc  \n",
      "0   0.883146   0.358209  0.727273  0.480000  0.940497  0.602461  \n",
      "1   0.760067   0.448430  0.833333  0.583090  0.879884  0.699897  \n",
      "2   0.717791   0.529661  0.822368  0.644330  0.859382  0.768480  \n",
      "3   0.744292   0.518519  0.711864  0.600000  0.819915  0.675307  \n",
      "4   0.887850   0.666667  0.588235  0.625000  0.877124  0.680508  \n",
      "5   0.772563   0.543478  0.704225  0.613497  0.874197  0.714355  \n",
      "6   0.802610   0.600000  0.877193  0.712589  0.905612  0.800645  \n",
      "7   0.724936   0.471264  0.845361  0.605166  0.823118  0.650027  \n",
      "8   0.694933   0.431416  0.836910  0.569343  0.837196  0.664848  \n",
      "9   0.824663   0.352941  0.750000  0.480000  0.906626  0.599935  \n",
      "10  0.885117   0.492958  0.813953  0.614035  0.940287  0.660140  \n",
      "11  0.780933   0.516304  0.833333  0.637584  0.893371  0.805359  \n",
      "12  0.764851   0.435897  0.906667  0.588745  0.902047  0.708860  \n",
      "13  0.810765   0.505000  0.882096  0.642289  0.920242  0.774295  \n",
      "14  0.796345   0.677419  0.875000  0.763636  0.905306  0.868095  \n",
      "15  0.750000   0.474074  0.831169  0.603774  0.859600  0.632968  \n",
      "16  0.704595   0.447368  0.918919  0.601770  0.868702  0.723571  \n",
      "17  0.871901   0.567164  0.535211  0.550725  0.849504  0.584094  \n",
      "18  0.878099   0.591667  0.876543  0.706468  0.931685  0.806946  \n",
      "19  0.804598   0.209877  0.809524  0.333333  0.923839  0.620933  \n",
      "\n",
      "[20 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Para ler o caminho do excel\n",
    "    caminho_excel = r\"C:\\Users\\JacyzinGuilherme(Bip\\mentoria-bip\\dados_editados\\australia_clima_v9.xlsx\"\n",
    "\n",
    "    \n",
    "    # Para ler o excel\n",
    "    df = pd.read_excel(caminho_excel)\n",
    "\n",
    "    # Rodar estrutura de treinamento\n",
    "    df_metricas_cidades = treinar_xgb_por_cidade_com_tuning(\n",
    "        df=df,\n",
    "        col_cidade=\"localidade\",\n",
    "        col_data=\"data\",\n",
    "        col_target=\"chove_amanha_vtr\",\n",
    "        proporcao_treino=0.8,\n",
    "        min_linhas_cidade=500,\n",
    "        n_iter=100,               \n",
    "        n_splits_tss=4,\n",
    "        scoring_valid=\"recall_accuracy\",\n",
    "        usar_threshold_otimo=True,\n",
    "        metrica_threshold=\"recall_accuracy\",\n",
    "        random_state=42\n",
    "    )\n",
    "    print(df_metricas_cidades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d205903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_metricas_cidades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f59bcafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saida_csv = r\"C:\\Users\\JacyzinGuilherme(Bip)\\mentoria-bip\\dados_editados\\metricas_por_cidade_xgb_v3.csv\"\n",
    "\n",
    "df_metricas_cidades.to_csv(r\"C:\\Users\\JacyzinGuilherme(Bip\\mentoria-bip\\dados_editados\\metricas_por_cidade_xgb_v2.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfdb389",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
